{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.12"
    },
    "colab": {
      "name": "text_sentiment_ngrams.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJgOv00dQHNj"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cf6MEqqnQHNn"
      },
      "source": [
        "\n",
        "Text classification with the torchtext library\n",
        "==================================\n",
        "\n",
        "using the torchtext library to build the dataset for the text classification analysis from tutorial https://pytorch.org/tutorials/beginner/text_sentiment_ngrams_tutorial.html#access-to-the-raw-dataset-iterators.\n",
        "\n",
        "   - Access to the raw data as an iterator\n",
        "   - Build data processing pipeline to convert the raw text strings into ``torch.Tensor`` that can be used to train the model\n",
        "   - Shuffle and iterate the data with `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gx7nleJqQHNn"
      },
      "source": [
        "Access to the raw dataset iterators\n",
        "-----------------------------------\n",
        "\n",
        "iterating the raw data as a tuple of label and text\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8pYxZFqQHNo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e1a75eb-9ed3-4a9c-fb07-3e6ec26e9cfb"
      },
      "source": [
        "import torch\n",
        "from torchtext.datasets import YelpReviewPolarity\n",
        "train_iter = YelpReviewPolarity(split='train')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "yelp_review_polarity_csv.tar.gz: 166MB [00:03, 51.1MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvESk_urWH5q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "870123ee-29a4-4086-a3fb-c85c2f9fdd0d"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1,\n",
              " \"Unfortunately, the frustration of being Dr. Goldberg's patient is a repeat of the experience I've had with so many other doctors in NYC -- good doctor, terrible staff.  It seems that his staff simply never answers the phone.  It usually takes 2 hours of repeated calling to get an answer.  Who has time for that or wants to deal with it?  I have run into this problem with many other doctors and I just don't get it.  You have office workers, you have patients with medical needs, why isn't anyone answering the phone?  It's incomprehensible and not work the aggravation.  It's with regret that I feel that I have to give Dr. Goldberg 2 stars.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qg_mXiRZQHNo"
      },
      "source": [
        "Prepare data processing pipelines\n",
        "---------------------------------\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Users can have a customized vocab by setting up arguments in the constructor of the Vocab class. For example, the minimum frequency ``min_freq`` for the tokens to be included [from tutorial].\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-d_PcLXQHNo"
      },
      "source": [
        "from torchtext.data.utils import get_tokenizer\n",
        "from collections import Counter\n",
        "from torchtext.vocab import Vocab\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = YelpReviewPolarity(split='train')\n",
        "counter = Counter()\n",
        "for (label, line) in train_iter:\n",
        "    counter.update(tokenizer(line))\n",
        "vocab = Vocab(counter, min_freq=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDq92SLaXL1Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8bc6a34d-5628-4b63-8679-7945b3359d2a"
      },
      "source": [
        "[vocab[token] for token in ['here', 'is', 'an', 'example']]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[52, 15, 69, 2031]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaLr885wQHNp"
      },
      "source": [
        "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOO-loexXRKG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a624e94-14bf-4fd8-9630-da17335abdc6"
      },
      "source": [
        "text_pipeline('here is the an example')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[52, 15, 3, 69, 2031]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iw9tAUSWQHNp"
      },
      "source": [
        "Generate data batch and iterator \n",
        "--------------------------------\n",
        "[from tutorial]\n",
        "\n",
        "`torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n",
        "is recommended for PyTorch users (a tutorial is `here <https://pytorch.org/tutorials/beginner/data_loading_tutorial.html>`__).\n",
        "It works with a map-style dataset that implements the ``getitem()`` and ``len()`` protocols, and represents a map from indices/keys to data samples. It also works with an iterable datasets with the shuffle argumnent of ``False``.\n",
        "\n",
        "Before sending to the model, ``collate_fn`` function works on a batch of samples generated from ``DataLoader``. The input to ``collate_fn`` is a batch of data with the batch size in ``DataLoader``, and ``collate_fn`` processes them according to the data processing pipelines declared previouly. Pay attention here and make sure that ``collate_fn`` is declared as a top level def. This ensures that the function is available in each worker.\n",
        "\n",
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of ``nn.EmbeddingBag``. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of indidividual text entries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pgBAoVTHQHNq"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
        "\n",
        "train_iter = YelpReviewPolarity(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fPnZSv6QHNq"
      },
      "source": [
        "Define the model\n",
        "----------------\n",
        "[from tutorial]\n",
        "\n",
        "The model is composed of the `nn.EmbeddingBag <https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag>`__ layer plus a linear layer for the classification purpose. ``nn.EmbeddingBag`` with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
        "\n",
        "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
        "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
        "performance and memory efficiency to process a sequence of tensors.\n",
        "\n",
        "![](https://github.com/pytorch/tutorials/blob/gh-pages/_downloads/_static/img/text_sentiment_ngrams_model.png?raw=1)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05dtUwvDQHNq"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SnzAPCXVQHNr"
      },
      "source": [
        "train_iter = YelpReviewPolarity(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yfTglmJgfeMq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5fe01d38-ee2e-45f4-e4d0-dbf2a7691910"
      },
      "source": [
        "num_class"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWJWpXnHQHNr"
      },
      "source": [
        "Define functions to train the model and evaluate results.\n",
        "---------------------------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjnsS0bDQHNr"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyswkGUuQHNr"
      },
      "source": [
        "Split the dataset and run the model\n",
        "-----------------------------------\n",
        "\n",
        "splitting the training\n",
        "dataset into train/valid sets with a split ratio of 0.95 (train) and\n",
        "0.05 (valid). Here we use\n",
        "`torch.utils.data.dataset.random_split <https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split>`__\n",
        "function in PyTorch core library.\n",
        "\n",
        "`CrossEntropyLoss <https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss>`__\n",
        "criterion combines ``nn.LogSoftmax()`` and ``nn.NLLLoss()`` in a single class.\n",
        "It is useful when training a classification problem with C classes.\n",
        "`SGD <https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html>`__\n",
        "implements stochastic gradient descent method as the optimizer. The initial\n",
        "learning rate is set to 5.0.\n",
        "`StepLR <https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR>`__\n",
        "is used here to adjust the learning rate through epochs.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Oy1gxi0QHNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb159a1b-1651-4525-9ffb-96554738fc3f"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = YelpReviewPolarity()\n",
        "train_dataset = list(train_iter)\n",
        "test_dataset = list(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 8313 batches | accuracy    0.775\n",
            "| epoch   1 |  1000/ 8313 batches | accuracy    0.863\n",
            "| epoch   1 |  1500/ 8313 batches | accuracy    0.877\n",
            "| epoch   1 |  2000/ 8313 batches | accuracy    0.890\n",
            "| epoch   1 |  2500/ 8313 batches | accuracy    0.897\n",
            "| epoch   1 |  3000/ 8313 batches | accuracy    0.897\n",
            "| epoch   1 |  3500/ 8313 batches | accuracy    0.899\n",
            "| epoch   1 |  4000/ 8313 batches | accuracy    0.898\n",
            "| epoch   1 |  4500/ 8313 batches | accuracy    0.905\n",
            "| epoch   1 |  5000/ 8313 batches | accuracy    0.904\n",
            "| epoch   1 |  5500/ 8313 batches | accuracy    0.907\n",
            "| epoch   1 |  6000/ 8313 batches | accuracy    0.909\n",
            "| epoch   1 |  6500/ 8313 batches | accuracy    0.907\n",
            "| epoch   1 |  7000/ 8313 batches | accuracy    0.912\n",
            "| epoch   1 |  7500/ 8313 batches | accuracy    0.913\n",
            "| epoch   1 |  8000/ 8313 batches | accuracy    0.912\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time: 92.83s | valid accuracy    0.922 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  1000/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  1500/ 8313 batches | accuracy    0.914\n",
            "| epoch   2 |  2000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  2500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  3000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  3500/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  4000/ 8313 batches | accuracy    0.917\n",
            "| epoch   2 |  4500/ 8313 batches | accuracy    0.921\n",
            "| epoch   2 |  5000/ 8313 batches | accuracy    0.918\n",
            "| epoch   2 |  5500/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  6000/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  6500/ 8313 batches | accuracy    0.916\n",
            "| epoch   2 |  7000/ 8313 batches | accuracy    0.919\n",
            "| epoch   2 |  7500/ 8313 batches | accuracy    0.920\n",
            "| epoch   2 |  8000/ 8313 batches | accuracy    0.919\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time: 92.42s | valid accuracy    0.904 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  1000/ 8313 batches | accuracy    0.930\n",
            "| epoch   3 |  1500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch   3 |  2500/ 8313 batches | accuracy    0.934\n",
            "| epoch   3 |  3000/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  3500/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch   3 |  4500/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  5000/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  5500/ 8313 batches | accuracy    0.931\n",
            "| epoch   3 |  6000/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  6500/ 8313 batches | accuracy    0.936\n",
            "| epoch   3 |  7000/ 8313 batches | accuracy    0.933\n",
            "| epoch   3 |  7500/ 8313 batches | accuracy    0.932\n",
            "| epoch   3 |  8000/ 8313 batches | accuracy    0.933\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time: 95.68s | valid accuracy    0.931 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 8313 batches | accuracy    0.936\n",
            "| epoch   4 |  1000/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  1500/ 8313 batches | accuracy    0.937\n",
            "| epoch   4 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  2500/ 8313 batches | accuracy    0.932\n",
            "| epoch   4 |  3000/ 8313 batches | accuracy    0.930\n",
            "| epoch   4 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch   4 |  4000/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  4500/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  5000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  5500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  6500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  7000/ 8313 batches | accuracy    0.933\n",
            "| epoch   4 |  7500/ 8313 batches | accuracy    0.934\n",
            "| epoch   4 |  8000/ 8313 batches | accuracy    0.934\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time: 94.09s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 8313 batches | accuracy    0.936\n",
            "| epoch   5 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  2000/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  2500/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  3000/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  4000/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  4500/ 8313 batches | accuracy    0.934\n",
            "| epoch   5 |  5000/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  5500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  6000/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  6500/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  7000/ 8313 batches | accuracy    0.935\n",
            "| epoch   5 |  7500/ 8313 batches | accuracy    0.933\n",
            "| epoch   5 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time: 94.22s | valid accuracy    0.932 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  1000/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   6 |  2000/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  3000/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  5000/ 8313 batches | accuracy    0.937\n",
            "| epoch   6 |  5500/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  6000/ 8313 batches | accuracy    0.935\n",
            "| epoch   6 |  6500/ 8313 batches | accuracy    0.937\n",
            "| epoch   6 |  7000/ 8313 batches | accuracy    0.937\n",
            "| epoch   6 |  7500/ 8313 batches | accuracy    0.936\n",
            "| epoch   6 |  8000/ 8313 batches | accuracy    0.936\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time: 93.90s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  1000/ 8313 batches | accuracy    0.937\n",
            "| epoch   7 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  2000/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  2500/ 8313 batches | accuracy    0.937\n",
            "| epoch   7 |  3000/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  3500/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  4000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  4500/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  5000/ 8313 batches | accuracy    0.936\n",
            "| epoch   7 |  5500/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  6000/ 8313 batches | accuracy    0.934\n",
            "| epoch   7 |  6500/ 8313 batches | accuracy    0.937\n",
            "| epoch   7 |  7000/ 8313 batches | accuracy    0.939\n",
            "| epoch   7 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch   7 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time: 95.30s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  1000/ 8313 batches | accuracy    0.935\n",
            "| epoch   8 |  1500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  2000/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  2500/ 8313 batches | accuracy    0.934\n",
            "| epoch   8 |  3000/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  3500/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  4000/ 8313 batches | accuracy    0.933\n",
            "| epoch   8 |  4500/ 8313 batches | accuracy    0.935\n",
            "| epoch   8 |  5000/ 8313 batches | accuracy    0.938\n",
            "| epoch   8 |  5500/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  6000/ 8313 batches | accuracy    0.937\n",
            "| epoch   8 |  6500/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  7000/ 8313 batches | accuracy    0.936\n",
            "| epoch   8 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch   8 |  8000/ 8313 batches | accuracy    0.938\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time: 95.22s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  1500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  2000/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  2500/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  3000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  3500/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch   9 |  4500/ 8313 batches | accuracy    0.938\n",
            "| epoch   9 |  5000/ 8313 batches | accuracy    0.938\n",
            "| epoch   9 |  5500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  6000/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  6500/ 8313 batches | accuracy    0.937\n",
            "| epoch   9 |  7000/ 8313 batches | accuracy    0.934\n",
            "| epoch   9 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch   9 |  8000/ 8313 batches | accuracy    0.938\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time: 95.13s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 8313 batches | accuracy    0.938\n",
            "| epoch  10 |  1000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  1500/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  2000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  2500/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  3000/ 8313 batches | accuracy    0.934\n",
            "| epoch  10 |  3500/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  4000/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  4500/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  5000/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  5500/ 8313 batches | accuracy    0.938\n",
            "| epoch  10 |  6000/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  6500/ 8313 batches | accuracy    0.937\n",
            "| epoch  10 |  7000/ 8313 batches | accuracy    0.936\n",
            "| epoch  10 |  7500/ 8313 batches | accuracy    0.935\n",
            "| epoch  10 |  8000/ 8313 batches | accuracy    0.935\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time: 95.26s | valid accuracy    0.933 \n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SsbRLEaxQHNt"
      },
      "source": [
        "Evaluate the model with test dataset\n",
        "------------------------------------\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJpTvh0EQHNu"
      },
      "source": [
        "Checking the results of the test dataset…\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4JpbHrpQHNu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23d6a669-8225-40cb-db7d-545f062d98f9"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fJ5RRMuSQHNv"
      },
      "source": [
        "Test on a random review\n",
        "---------------------\n",
        "\n",
        "Use the best model so far and test a positive restraurant review.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oYWT6ecQHNv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71567ca8-cf3d-40e7-ee1b-83dd9c60579d"
      },
      "source": [
        "yelp_polarity_label = {1: \"Negative\",\n",
        "                 2: \"Positive\",\n",
        "                }\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"They have the best happy hours, the food is good, and service is even better. When it is winter we become regulars\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s sentence\" %yelp_polarity_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Positive sentence\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3_LeDXooml4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}